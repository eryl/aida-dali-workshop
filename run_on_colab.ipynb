{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD-kpPhceZRA"
      },
      "source": [
        "# ! Remember to set the runtime type to GPU !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuuX9GrhY64D"
      },
      "source": [
        "# NVIDIA DALI on Colab\n",
        "\n",
        "We will use this notebook to execute shell commands.\n",
        "The first thing we do is clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIEVkhODY_-L",
        "outputId": "c1498b8f-5b88-41ef-9f35-c3f5d9573da8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/eryl/aida-dali-workshop.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThNoZJ2mYYXW",
        "outputId": "de3d07ef-0f29-486b-b593-7db141105094"
      },
      "outputs": [],
      "source": [
        "!git -C /content/aida-dali-workshop/ pull   # update the codebase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScF6hECcd_jQ"
      },
      "outputs": [],
      "source": [
        "!pip install monai nibabel > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-Nmf5PToek"
      },
      "source": [
        "Now check which version of cuda is installed by running the command below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qSNSDwcTk0F"
      },
      "outputs": [],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPl93R1dTtcj"
      },
      "source": [
        "When we ran this, it says CUDA 12.2, so that's the DALI version we'll install (dali targets the major revisions, so either 11.0 or 12.0, expressed as 110 or 120)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9jgEXxrTzWk",
        "outputId": "8897c466-3537-4279-f55a-ec4cbe0176ef"
      },
      "outputs": [],
      "source": [
        "!pip install nvidia-dali-cuda120"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQmo8z3ZUAIf"
      },
      "source": [
        "## Running scripts\n",
        "The code we clone is in /content/aida-dali-workshop. We'll start by making that our working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1qWprp7T_Sv",
        "outputId": "5ae99ec5-8422-401f-a43a-cc8e3fafbf14"
      },
      "outputs": [],
      "source": [
        "%cd /content/aida-dali-workshop/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfubncuMULKH"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwGEK7MgUQbG"
      },
      "source": [
        "## The first training script\n",
        "We can now run the first training script. It will download the image dataset we'll use (the Oxford IIIT pets dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv3Oub7PUMFV"
      },
      "outputs": [],
      "source": [
        "!python examples/train_pet_resnet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzzcbeRxVhPf"
      },
      "source": [
        "What we're mostly interested in this case is how long the training epoch takes compared to our other data loading methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrTJtRNwV0OE"
      },
      "source": [
        "## The DALI version\n",
        "Now try to run the DALI version and compare the training time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkDcepDFV0OK"
      },
      "outputs": [],
      "source": [
        "!python examples/train_pet_resnet_dali.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNPCJeJJWolh"
      },
      "source": [
        "Did you notice any difference in time it took to process the batches (how many iterations per second did the two methods achieve)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sVZB9x2W1fV"
      },
      "source": [
        "## Pre-augmenting the data\n",
        "We've looked at how we can offload the augmentation to the GPU using DALI. As we get more powerful GPUs this will probably become more important to gain speedups in utilization.\n",
        "\n",
        "Another way to speed up dataloading is to perform the augmentation ahead of time. This only makes sense if you have plenty storage and will be using the training dataset to train multiple models, but if you plan to run large amounts of cross validation you will likely see significant speedups.\n",
        "\n",
        "One downside to this method is that we need to generate different augmentations for each epoch (the core idea of data augmentations is that the exact same image should never occur multiple times in the training data). This means that storage requierments will increase with the planned number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-fUgdSFWyj2"
      },
      "outputs": [],
      "source": [
        "!python examples/train_pet_resnet_preaugmented.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJaAG4CQUWCw"
      },
      "outputs": [],
      "source": [
        "! du -sh data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Zxncb4MY1co"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC_zabzNi3Ad"
      },
      "source": [
        "# Hands-on\n",
        "\n",
        "As a hands-on session, we will try to adapt an existing pytorch script to using DALI. Here you can chose to work on an experiment of your own, or try the 3D unet supplied in the examples (see below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fodQ1I_jDqq"
      },
      "source": [
        "## 3D Unet\n",
        "In this repository, there is a script called `examples/unet_training_array.py` which is taken from the monai examples. Here the challange is to take the existing training data augmentation pipeline and try to convert it to a DALI pipeline. While the resnet training example should serve as a rough sketch, the challange here will be in defining data augmentation steps which match those used by the original script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cBpa5u4jfm4",
        "outputId": "a1d4920f-d470-4fd9-a541-d9857f4a2357"
      },
      "outputs": [],
      "source": [
        "# First we create a synthetic segmentation dataset\n",
        "!python examples/unet_create_dataset.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbvORR4XkDsJ",
        "outputId": "233494cb-55ac-4def-deea-2984340b307d"
      },
      "outputs": [],
      "source": [
        "# Now we can run the training script\n",
        "!python examples/unet_training_array.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFQY01hAkj4k"
      },
      "source": [
        "## Adapting to the DALI pipeline\n",
        "\n",
        "Now try changing this pipeline to use NVIDIA dali for the training data loader. You will need to wrap the ImageDataset used in the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGtRDaN0kRbb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
